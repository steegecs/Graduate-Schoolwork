# Sharing-Repository
("Optimization of GHG emissions through meal planning.PDF") -----> 
For this project, we were instructed to describe a problem which can be solved through optimization, conduct and literature review on the topic, propose a linear model complete with linear constraints, decision variables, semi-simluated data, and an objective function, and present comptuation results and model validation. Our chosen optimization topic was on the reduction of greenhouse gas emissions with school lunch items as decisions variables with the goal of meeting various financial, nutritional, allergenic, and preferential constraints about lunch item repetition. 

("PCA, SVD, LDA Assignment.pdf") -----> 
This file is a demonstration of EDA, principal component analysis, singular value decomposition, and linear discriminant analysis in Python
for a class project across multiple data sets. The purpose was to gain intuition about how and why they work and also to understand when their use is appropriate. After completing this project, I better understood how dimensionality reduction methods like PCA and SVD are effective in capturing the most variance with a limited number components - leading to more efficient computation and sometimes clearer and more effective insights. I also gained a better intuition about how PCA and SVD are limited as classifiers. LDA does a much better job as a classifier, because it chooses a 1 dimensional vector that best allows separation between the desired classes and is optimized for this purpose by its objective function. 

("Frequent Pattern Mining.PDF") ----->
This file is a demonstration of Frequent Pattern mining algorithms which include algorithms such as Apriori, ECLAT, and FPGrowth. Compared the computational complexity of these algorithms in computing frequent subsets at different support thresholds. This excercise highlighted the computational complexity advantages of ECLAT and FPGrowth that take off at low support thresholds. Substring and subsequence mining methods were explored using the variation of these algorithms for subsequence mining and substring mining using suffix trees. This excercise highlighted the key differences and intuitions substring and subsequence mining. 

("Clustering.PDF") ----->
This file demonstrates various techniques in clustering including k-means, agglomerative (single link, max link, average link), density-based, and spectral clustering. The purpose was to explore situations in which each clustering algorithms perform well or when another algorithm is better suited and why this is the case. The goal was to also to compare these algorthms with each other and understand that there is no one magic clustering algorithm - it is useful to have multiple at your disposal for a given set of data. I also explored clustering tendency using the Hopkin's Statistic to gain intuition about how it works and how it can be useful in determining if clustering will likely be an effective tool. Finally, there is an application of clustering to a dataset concerning tumor classification. The purpose here was to learn to use these algorithms and properly tune paramaters according to the silhouette coefficient for k-means or by the number of clusters created for density-based clustering. 

("Classification.PDF") ----->
In this excercise, I demonstrate classification algorithms such as decision trees, KNN, Naive Bayes, and both linear and non-linear Support Vector Machines. The purpose here was to use visualizations to gain intuition about what kind of decision boundaries you can get from using each of these algorthms. Some of them are linear and others are non-linear whilst decision trees draw strictly axis parallel decision boundaries. I comparatively analyzed these algorithms and saw how some perform better than others an certain scenarios and which ones tend to be the most effective in a general sense. One goal was to explore how tuning the parameters of these algorthms affect clustering. I then proceeded to demonstrate clustering evaluation using accuracy and precision measurements of the predictions. I also explored ensemble methods like bagging and then saw how gradient boosting methods can be used to improve the speed and consistency of classification accuracy as you increase the number of bags. In addition to this comparison, I provide a detailed analysis the mean and standard deviation of accuracy using bagging and boosting with how they fluctuate as the number of bags increases. Finally, these techniques were applied to a real dataset about benign vs. malignant tumor classification. The goal here was to highlight the importance of evaluation metrics and how, depending on what you are trying to classify, it may be important to give more weight to either the sensitivity or specificity of your classification.

("The Study of Wine Quality.R"
"Wine Presentation - pdf.pdf") ----->
The last two files are also from class projects. The purpose of these projects were to learn visualize and describe the variables of a data. 
Specifically, we wanted understand how skewed distributions affects summary statistics and when to suspect of outliers that could compromise
data integrity. I created a power point presentation to make scrolling the the summary of each variable better and easier to read through. 

("Wine.Rmd") -----> 
In this excercise. Were instructed to use our understanding of CLT, Bootstrapping, and Maximum Likelihood Estimation to estimate parameters for mean, standard devation, and proportion and then create confidence intervals for our estimates.  I find it fascinating that there are multiple way to estimate these parameters, and I especially enjoyed this section of the wine project. My favorite part was using my understand of probability distributions by implementing a binomial distribution with maximum likelihood estimate to estimate the proportion of excellent vs. not excellent wines. So cool! 

("FAA Final.pdf") ----->
The purpose of this project was to become familiar with the process of statistical computing. This process was a practice in combining data sets, variable completeness checking, validity checking, cleaning, data summarization, visualization, and finally correlation and statistical analysis with regression and t-test. I enjoyed this project because it has helped better integrate the steps involved in data analysis into how I think about approaching analytical projects. I also gained more insight as to how to deal with missing data or data that takes on an abnormal value. This has been an important project in my journey as an aspiring data scientist. 


